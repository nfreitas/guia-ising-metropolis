\documentclass[a4paper,11pt,oneside]{article}
\pagestyle{plain}
%\usepackage[a4paper, margin=1.2in]{geometry}

\usepackage{algpseudocode}

%configuración del idioma y la codificacion
\usepackage[spanish]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%fuente
%\usepackage{times}
\usepackage{palatino}

%math
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{bbold}
\usepackage{dsfont}
\usepackage{braket,mleftright}

\usepackage{graphicx}
\usepackage{subcaption}

%otros paquetes
%\usepackage{rotating}
\usepackage{cite}
\usepackage[linktoc=page]{hyperref}
\usepackage{indentfirst}

%comandos
\newcommand{\mean}[1]{\langle #1 \rangle}

\title{Práctica computacional - Modelo de Ising}
\author{}
\date{}

\begin{document}
\maketitle

\section{Introducción}

Consideremos una grilla de $N = L\times L$ sitios en cada uno de los cuales hay
un spin que solo puede tomar los valores $S = 1,-1$. Estos
spines tienen interacciones tipo Ising dadas por el siguiente Hamiltoniano:
\begin{equation}
    H = -J\sum_\text{n.n} S_i S_j - B \sum_i S_i
\end{equation}
Donde la primer suma es sobre primeros vecinos y $J$ y $B$ son parámetros que
dan la energía de interacción entre spines, y entre cada uno de ellos y un
campo magnético externo. Supongamos que queremos calcular numéricamente algún
observable macroscópico a una dada temperatura $\beta^{-1}$, como por ejemplo
el valor medio de la magnetización total:
\begin{equation}
    \mean{M} = \sum_{S_1,\dots,S_N} p_{S_1,\dots,S_N} \; (S_1+S_2+\dots+S_N).
    \label{eq:mag_total}
\end{equation}
Esta vez la suma es sobre todas las configuraciones posibles de los spines, y
$p_{S_1,\dots,S_N}$ es la probabilidad de cada configuración, que según el
ensamble canónico, es:
\begin{equation}
    p_{S_1,\dots,S_N} = \frac{1}{Z} \; e^{-\beta H(S_1,\dots,S_N)},
\end{equation}
donde $Z$ es la función de partición:
\begin{equation}
    Z = \sum_{S_1,\dots,S_N} e^{-\beta H(S_1,\dots,S_N)}.
\end{equation}
Si quisieramos evaluar la fórmula de la ecuación \ref{eq:mag_total} tal cual esta planteada,
necesitariamos recorrer todos los estados posibles del sistema (al igual que
para calcular exactamente la función de partición, necesaria para calcular las
probabilidades $p_{S_1,\cdots,S_N}$). El problema es que la cantidad de estados
es $2^{N}$, y solo para $N=100$ este número es ridículamente grande. Ninguna
computadora podría recorrer todos estos estados en un tiempo razonable. Pero,
como sabemos, hacer eso tampoco es necesario, ya que solo una fracción de los
estados posibles contribuirá apreciablemente a la suma en la ecuación
\ref{eq:mag_total}. Entonces, dado que no podemos recorrer todos los estados,
una estrategia posible sería solo considerar algunos términos de la ecuación
\ref{eq:mag_total}, de una forma tal que estos términos sean representativos
de los estados que típicamente recorre el sistema cuando se encuentra a
temperatura $\beta^{-1}$. Estos términos, naturalmente, son aquellos para los
cuales la probabilidad $p_{S_1,\dots,S_N}$ es mayor.
Dicho de otra forma, necesitamos `muestrear' la
distribución de probabilidad $p_{S_1,\dots,S_N}$, es decir generar estados de
forma tal que la fracción de veces que se genera el estado $(S_1,\dots,S_N)$
sea $p_{S_1,\dots,S_N}$. El algoritmo de Metrópolis, que pertenece a la familia
mas general de algoritmos de Montecarlo, es una solución a este problema.

\section{Algoritmo de Metrópolis}

El algoritmo de Metrópolis genera estados secuencialmente, de forma que cada
estado es generado a partir del anterior de forma estocástica. La secuencia de
estados generados es entonces una cadena de Markov, y el proceso de generación
es tal que la distribución estacionaria es la distribución de Boltzmann que se
desea muestrear. El algoritmo es muy sencillo, y para el caso de un sistema de
Ising 2D se describe de la siguiente manera:
\begin{enumerate}
\item Se selecciona un estado inicial al azar o con algún criterio. Este estado
    se uarda en una matriz $S$ de $L\times L$ cuyos elementos son $1$ o $-1$.
\item Se elige aleatoriamente un sitio de la red. Es decir se eligen
    coordenadas $i$ y $j$ según una distribución uniforme entre $1$ y $L$ (o
    entre $0$ y $L-1$, según como se indexen las matrices en el lenguaje que
    usen)
\item Se calcula la diferencia de energía $\Delta E$ que resultaría de invertir
    el estado del spin en la posición $(i,j)$.
\item Si $\Delta E \leq 0$, se invierte el spin, es decir que el nuevo estado
    de la red es $S'$, que es igual a $S$ excepto en el sitio $(i,j)$,
    donde $S'(i,j) = -S(i,j)$.
\item Si $\Delta E >0$, el spin se invierte con probabilidad $p = e^{-\beta\Delta
    E}$. El nuevo estado de la red es de nuevo $S'$, que con probabilidad
    $1-p$ será igual a $S$, y con probabilidad $p$ sera distinto sólo en el
    valor del sitio $(i,j)$.
\item Se repiten los pasos 2-5 con el nuevo estado $S'$, tantas veces como sea
    necesario.
\end{enumerate}
Este algoritmo genera una secuencia de estados $S_1,S_2,S_3,\dots$, donde cada
estado depende solo del anterior de una manera estocástica (un dado estado
$S_k$ puede dar lugar a muchos estados distintos $S_{k+1}$ con distintas probabilidades).
El punto crucial del algoritmo es que las probabilidades de transición de un
estado $a$ a un estado $b$ satisfacen la condición:
\begin{equation}
    \frac{P(a\to b)}{P(b\to a} = e^{-\beta(E_b-E_a)}
    \label{eq:bal_detallado}
\end{equation}

\end{document}


